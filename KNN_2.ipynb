{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1hWj7+xoaR73LuXNU9rNl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shallynagfase9/KNN-PCA/blob/main/KNN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
      ],
      "metadata": {
        "id": "82loxOsKsO8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Euclidean Distance-\n",
        "Characteristics:\n",
        "- Represents the straight-line distance between two points.\n",
        "- Takes into account both the magnitude and direction of differences across all dimensions.\n",
        "- Sensitive to the scale of features (features with larger ranges can dominate the distance calculation).\n",
        "\n",
        "Manhattan Distance-\n",
        "Characteristics:\n",
        "- Represents the distance between two points measured along axes at right angles (like navigating through city blocks).\n",
        "- Considers only the absolute differences along each dimension.\n",
        "- Less sensitive to outliers compared to Euclidean distance.\n",
        "\n",
        "While both Euclidean and Manhattan distances are effective distance metrics for KNN, their differences in sensitivity to feature scale and geometric\n",
        "interpretation can lead to different performances in classification or regression tasks depending on the nature of the data and the problem at hand.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NjfYJo_esR8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?"
      ],
      "metadata": {
        "id": "lfQLMHlxsSP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "KNN or k nearest neighbor is a non-parametric, supervised learning classifier, that can be used for both classification and regression tasks, which uses proximity as a feature for classification or prediction.\n",
        "It is a classic example of a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.\n",
        "\n",
        "Using K-Nearest Neighbor, we predict the category of the test point from the available class labels by finding the distance between the test point and trained k nearest feature values.\n",
        "\n",
        "Here are several techniques commonly used to determine the optimal k value\n",
        "- Grid Search with Cross-Validation\n",
        "- Elbow Method\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Ww1fjz1psVR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?"
      ],
      "metadata": {
        "id": "gdoGKTWIsVfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The choice of distance metric should be guided by the nature of your data and the requirements of your problem.\n",
        "Understanding how each metric calculates distance and its implications on the performance of KNN can help you make an informed decision.\n",
        "\n",
        "Hereâ€™s how different distance metrics affect the model and when you might prefer one over the other:\n",
        "\n",
        "Common Distance Metrics:\n",
        "- Euclidean Distance\n",
        "- Manhattan Distance\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jl8QUcKTsYAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?"
      ],
      "metadata": {
        "id": "IW9GXG9AsYYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Common Hyperparameters in KNN:\n",
        "\n",
        "- k (Number of Neighbors):\n",
        "- Distance Metric\n",
        "- Weight Function (for Prediction)\n",
        "- Algorithm for Neighbor Searc\n",
        "- Leaf Size (for KD-tree or Ball-tree)\n",
        "- Parallelization Options\n",
        "\n",
        "Hyperparameter Tuning Strategies:\n",
        "\n",
        "- Grid Search with Cross-Validation: Define a grid of hyperparameters (e.g., k, distance metric, weight function) and use cross-validation to evaluate each combination. Select the set of hyperparameters that yield the best cross-validation performance.\n",
        "\n",
        "- Random Search: Randomly sample hyperparameter combinations from predefined ranges. This approach can sometimes be more efficient than grid search, especially when the hyperparameter space is large.\n",
        "\n",
        "- Bayesian Optimization: Utilize Bayesian techniques to adaptively select hyperparameter combinations based on past evaluations. This method can efficiently explore the hyperparameter space and converge to good solutions with fewer evaluations compared to grid search.\n",
        "\n",
        "- Automated Hyperparameter Tuning Tools: Use libraries such as scikit-learn's GridSearchCV, RandomizedSearchCV, or tools like Optuna or Hyperopt for automated hyperparameter tuning.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "U8Ae6nlnsbj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?"
      ],
      "metadata": {
        "id": "-HVwbsBZsb2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Effects of Training Set Size:\n",
        "- Generalization\n",
        "- Bias-Variance Tradeoff\n",
        "- Computational Efficiency\n",
        "\n",
        "\n",
        "Techniques to Optimize Training Set Size:\n",
        "\n",
        "- Cross-Validation:\n",
        "Use techniques like k-fold cross-validation to assess model performance across different training set sizes.\n",
        "This allows you to evaluate how performance changes as you vary the size of the training set.\n",
        "\n",
        "- Learning Curves:\n",
        "Plot learning curves that show training and validation/test performance metrics against the size of the training set.\n",
        "Identify the point where increasing the training set size stops significantly improving model performance.\n",
        "\n",
        "- Resampling Techniques:\n",
        "If the dataset is large, consider random sampling or stratified sampling to create smaller, representative training sets.\n",
        "This can reduce computational overhead while maintaining model performance.\n",
        "\n",
        "- Data Augmentation (for KNN classifiers):\n",
        "Generate synthetic data points or augment existing data to increase the effective size of the training set.\n",
        "Techniques such as SMOTE (Synthetic Minority Over-sampling Technique) can be used to create new synthetic instances for imbalanced datasets.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bFqetycPsebw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?"
      ],
      "metadata": {
        "id": "20SMXCLWsepA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Drawbacks of KNN:\n",
        "- Computational Complexity\n",
        "Overcoming: To improve performance, use data structures like KD-trees or Ball trees for efficient nearest neighbor search. These data structures can significantly speed up the nearest neighbor search process.\n",
        "\n",
        "- Memory Usage\n",
        "Overcoming: Consider using approximate nearest neighbor methods like locality-sensitive hashing (LSH) or clustering-based methods to reduce the number of comparisons needed or to store only representative points.\n",
        "\n",
        "- Sensitivity to High-Dimensional Spaces\n",
        "Overcoming: Consider preprocessing techniques such as outlier detection and removal, or use more robust distance metrics (like Mahalanobis distance) that are less sensitive to outliers.\n",
        "\n",
        "- Imbalanced Data\n",
        "Overcoming: Use techniques such as over-sampling (e.g., SMOTE) or under-sampling to balance the class distribution, or use modified distance metrics (e.g., weighted distances) that give more importance to minority class instances.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "-sCP5SdqshNn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}