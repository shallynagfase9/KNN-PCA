{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLBb9XKFUUqqYq/MHxaNDf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shallynagfase9/KNN-PCA/blob/main/Dimensionality_Reduction_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q1. What is a projection and how is it used in PCA?"
      ],
      "metadata": {
        "id": "tlhKsdd6MbYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In the context of Principal Component Analysis (PCA), a projection refers to the process of transforming data from its original high-dimensional space to a new lower-dimensional space.\n",
        "This transformation is achieved by projecting the data onto a set of orthogonal axes (principal components) that capture the most variance in the data.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zGyppYp3Mdpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q2. How does the optimization problem in PCA work, and what is it trying to achieve?"
      ],
      "metadata": {
        "id": "TuyecCRDMdyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "The optimization problem in Principal Component Analysis (PCA) is focused on finding a set of orthogonal axes (principal components) onto which the original high-dimensional data can be projected in such a way that the variance of the projected data is maximized.\n",
        "This means PCA seeks to identify the directions along which the data varies the most.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "q8rVQ2RQMfiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q3. What is the relationship between covariance matrices and PCA?"
      ],
      "metadata": {
        "id": "dcDHZki0MftE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The relationship between covariance matrices and Principal Component Analysis (PCA) is central to understanding how PCA works. PCA leverages the covariance matrix to identify the principal components, which are the directions of maximum variance in the data.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tuX6MaanMhbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q4. How does the choice of number of principal components impact the performance of PCA?"
      ],
      "metadata": {
        "id": "9RH2iJ0bMhrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The choice of the number of principal components (PCs) in Principal Component Analysis (PCA) directly impacts its performance and the effectiveness of dimensionality reduction.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VVacydhJMjX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
      ],
      "metadata": {
        "id": "XmsvZcCUMjfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PCA is a powerful technique for feature selection that offers dimensionality reduction benefits, noise reduction, and improved model interpretability.\n",
        "It is particularly useful in scenarios where reducing the number of features while retaining meaningful information is desired.\n",
        "\n",
        "Benefits of Using PCA for Feature Selection\n",
        "- Dimensionality Reduction\n",
        "- Noise Reduction\n",
        "- Interpretability\n",
        "- Preprocessing Step\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N2SFZHJuMlNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q6. What are some common applications of PCA in data science and machine learning?"
      ],
      "metadata": {
        "id": "xe2BUWEgMlVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Principal Component Analysis (PCA) is a versatile technique with various applications in data science and machine learning.\n",
        "- Dimensionality Reduction\n",
        "- Data Visualization\n",
        "- Preprocessing\n",
        "- Pattern Recognition and Classification\n",
        "- Financial and Economic Analysis\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FcXWBuTCMnZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q7.What is the relationship between spread and variance in PCA?"
      ],
      "metadata": {
        "id": "wpYeWy3KMnk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In PCA, variance quantifies the amount of variability captured by each principal component, while spread describes how widely data points are distributed along these components.\n",
        "The relationship between them underscores the fundamental role of eigenvalues in determining the significance and representation of principal components in the transformed space.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kVn11emUMpm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q8. How does PCA use the spread and variance of the data to identify principal components?"
      ],
      "metadata": {
        "id": "Y7cmCELwMp5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PCA leverages the concepts of spread (how widely data points are distributed) and variance (amount of variability captured) to identify and rank principal components,\n",
        "enabling effective dimensionality reduction and data analysis in a wide range of fields including statistics, machine learning, and data science.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0HLUM0IJMrtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q9. How does PCA handle data with high variance in some dimensions but low variance in others?"
      ],
      "metadata": {
        "id": "u79sufJoMr1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Principal Component Analysis (PCA) handles data with high variance in some dimensions and low variance in others by emphasizing the dimensions (features) that contribute the most to the overall variability of the dataset.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gaCeLZLqOUqA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}