{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkwe9elW7wlHLZh41gHY9E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shallynagfase9/KNN-PCA/blob/main/KNN_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the KNN algorithm?"
      ],
      "metadata": {
        "id": "ceZvOU0JnDci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The K-Nearest Neighbors (KNN) algorithm is a simple, yet powerful supervised machine learning algorithm used for classification and regression tasks.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "85eBaCPWnGYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. How do you choose the value of K in KNN?"
      ],
      "metadata": {
        "id": "X1FrYmvPnGl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Choosing the value of K in the K-Nearest Neighbors (KNN) algorithm is crucial for its performance. The value of K determines the number of nearest neighbors that contribute to the prediction of a new data point.\n",
        "The k value in the k-NN algorithm defines how many neighbors will be checked to determine the classification of a specific query point. For example, if k=1, the instance will be assigned to the same class as its single nearest neighbor. Defining k can be a balancing act as different values can lead to overfitting or underfitting.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7ey_L4BgnIG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is the difference between KNN classifier and KNN regressor?"
      ],
      "metadata": {
        "id": "dIQ9ZQr8nIVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "KNN Classifier-\n",
        "- Purpose:\tClassifying data points into categories\n",
        "- Prediction:\tMajority vote of K nearest neighbors\n",
        "- Output:\tDiscrete class labels\n",
        "- Evaluation:\tAccuracy, precision, recall, F1-score\n",
        "- Decision Boundary:\tPiecewise linear, complex\n",
        "\n",
        "KNN Regressor-\n",
        "- Purpose: Predicting continuous values\n",
        "- Prediction: Average of values of K nearest neighbors\n",
        "- Output: Continuous values\n",
        "- Evaluation: MSE, RMSE, MAE, R²\n",
        "- Decision Boundary: Smooth function\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vvfrccz6nJ_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. How do you measure the performance of KNN?"
      ],
      "metadata": {
        "id": "iixqPR6fnKO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "- Classification Performance Metrics: Accuracy, confusion matrix, precision, recall, F1-score, ROC curve, and AUC.\n",
        "- Regression Performance Metrics: MSE, RMSE, MAE, and R².\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lsTZ175QnLyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is the curse of dimensionality in KNN?"
      ],
      "metadata": {
        "id": "jxd7cRmRnL7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces.\n",
        "When it comes to the K-Nearest Neighbors (KNN) algorithm, the curse of dimensionality can significantly affect its performance, visualization of data.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KJaMbRnbnNn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. How do you handle missing values in KNN?"
      ],
      "metadata": {
        "id": "WlWZCt0fnNxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Handling missing values is a crucial step in preparing data for the K-Nearest Neighbors (KNN) algorithm. Here are some strategies to handle missing values in KNN:\n",
        "\n",
        "1. Removing Instances with Missing Values\n",
        "2. Imputation Techniques\n",
        "3. Using Algorithms that Handle Missing Values Directly\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FStFddPunPvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
        "which type of problem?"
      ],
      "metadata": {
        "id": "IG_OvowjnP7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The K-Nearest Neighbors (KNN) algorithm can be applied to both classification and regression tasks.\n",
        "The performance of the KNN classifier and KNN regressor depends on the type of problem at hand, the nature of the data, and the specific evaluation metrics used.\n",
        "\n",
        "KNN Classifier-\n",
        "- Purpose:\tClassifying data points into categories\n",
        "- Prediction:\tMajority vote of K nearest neighbors\n",
        "- Output:\tDiscrete class labels\n",
        "- Evaluation:\tAccuracy, precision, recall, F1-score\n",
        "- Decision Boundary:\tPiecewise linear, complex\n",
        "\n",
        "KNN Regressor-\n",
        "- Purpose: Predicting continuous values\n",
        "- Prediction: Average of values of K nearest neighbors\n",
        "- Output: Continuous values\n",
        "- Evaluation: MSE, RMSE, MAE, R²\n",
        "- Decision Boundary: Smooth function\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "17LbXsMpnSyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
        "and how can these be addressed?"
      ],
      "metadata": {
        "id": "-__RKBdknS8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "The K-Nearest Neighbors (KNN) algorithm has several strengths and weaknesses when applied to classification and regression tasks. Here, we'll explore these in detail and discuss strategies to address the weaknesses.\n",
        "\n",
        "Strengths of KNN Algorithm:\n",
        "\n",
        "Classification :-\n",
        "- Simplicity: KNN is easy to understand and implement.\n",
        "- Non-parametric: It makes no assumptions about the underlying data distribution.\n",
        "- Versatility: Can handle multi-class classification problems and non-linear decision boundaries.\n",
        "- Adaptability: Naturally handles changes in data distribution and can be easily updated with new data points.\n",
        "\n",
        "Regression :-\n",
        "- Simplicity: KNN is straightforward and easy to implement.\n",
        "- Non-parametric: It doesn't assume a specific functional form for the relationship between features and the target.\n",
        "- Flexibility: Can model complex relationships in the data due to its non-parametric nature.\n",
        "- Locality: Predictions are made based on the local neighborhood, which can be advantageous in capturing local variations.\n",
        "\n",
        "Weaknesses of KNN Algorithm:\n",
        "\n",
        "Both Classification and Regression :-\n",
        "\n",
        "- Computational Cost: KNN requires storing the entire dataset and performing distance calculations for each prediction, making it computationally expensive, especially for large datasets.\n",
        "- Memory Intensive: KNN needs to store all training data, which can be problematic for large datasets.\n",
        "- Curse of Dimensionality: As the number of dimensions increases, the distance between data points becomes less meaningful, degrading performance.-\n",
        "- Sensitivity to Irrelevant Features: Irrelevant or noisy features can adversely affect performance.\n",
        "- Imbalanced Data: KNN can be biased towards the majority class in imbalanced datasets.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "qVXppRG7nTgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
      ],
      "metadata": {
        "id": "z1yGfSx4nT2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Euclidean Distance :-\n",
        "- Nature of Distance -\tStraight-line distance\n",
        "- Sensitivity to Scale - \tHigh (sensitive to large differences)\n",
        "- Outlier Sensitivity\t - Higher (squares differences)\n",
        "- Dimensional Suitability - \tBetter for low-dimensional, continuous spaces\n",
        "- Computational Complexity - \tTypically higher due to square root calculation\n",
        "\n",
        "Manhattan Distance :-\n",
        "- Nature of Distance -\tGrid-based (taxicab) distance\n",
        "- Sensitivity to Scale - Lower (linear contribution from each dimension)\n",
        "- Outlier Sensitivity\t - Lower (absolute differences)\n",
        "- Dimensional Suitability - Better for high-dimensional, categorical spaces\n",
        "- Computational Complexity - Typically lower due to simpler calculations\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Z0VCpDKxnYex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. What is the role of feature scaling in KNN?"
      ],
      "metadata": {
        "id": "XR6GIYdnnYrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Feature scaling is important in KNN to prevent features with larger magnitudes from dominating the distance calculation, ensure fair comparisons between features, improve convergence speed, and reduce the influence of outliers.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "g4bLZ-9hnaKq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}