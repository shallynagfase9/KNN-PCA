{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjXR8HHvUNZqdMXi4egF02",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shallynagfase9/KNN-PCA/blob/main/Dimensionality_Reduction_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach? Explain with an example."
      ],
      "metadata": {
        "id": "RZLTlQA2OhLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Eigenvalues and eigenvectors are fundamental concepts in linear algebra and are closely related to the eigen-decomposition approach, which is commonly used in various mathematical and computational applications, including Principal Component Analysis (PCA).\n",
        "\n",
        "Eigenvalues and Eigenvectors\n",
        "\n",
        "Eigenvectors:\n",
        "\n",
        "An eigenvector of a square matrix A is a non-zero vector v such that when A is multiplied by v, the result is a scalar multiple of v:\n",
        "Av = λv\n",
        "Here, λ is a scalar known as the eigenvalue corresponding to the eigenvector v.\n",
        "\n",
        "Eigenvalues:\n",
        "\n",
        "Eigenvalues are the scalars λ that satisfy the above equation for a given matrix A and eigenvector v.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jynKFdQQOkUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q2. What is eigen decomposition and what is its significance in linear algebra?"
      ],
      "metadata": {
        "id": "_KRIrASIOkkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Eigen decomposition, also known as spectral decomposition, is a fundamental concept in linear algebra that decomposes a square matrix into a set of eigenvalues and corresponding eigenvectors.\n",
        "This decomposition is pivotal in various mathematical and computational applications due to its ability to simplify matrix operations and reveal important structural properties of matrices.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nrtRrqjJOmSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the Eigen-Decomposition approach? Provide a brief proof to support your answer."
      ],
      "metadata": {
        "id": "X_0tBa-COmg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "For a square matrix to be diagonalizable using the eigen-decomposition approach, it must satisfy the following conditions:\n",
        "\n",
        "- Full Set of Linearly Independent Eigenvectors: The matrix A must have a complete set of n linearly independent eigenvectors, where n is the size of the matrix (number of rows or columns).\n",
        "- Geometric Multiplicity Equals Algebraic Multiplicity: For each eigenvalue 𝜆, the geometric multiplicity (dimension of the eigenspace associated with 𝜆) must equal its algebraic multiplicity (the number of times 𝜆iappears as a root of the characteristic polynomial of A).\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QRdNrm-_OpVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach? How is it related to the diagonalizability of a matrix? Explain with an example."
      ],
      "metadata": {
        "id": "bHWLARjaOppc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "The Spectral Theorem ensures that certain matrices (such as symmetric or normal matrices) can be diagonalized using their eigenvectors and eigenvalues. This diagonalization simplifies matrix operations, reveals fundamental properties of matrices, and plays a crucial role in various applications in mathematics, physics, and engineering.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "G6CTOgfLOtLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q5. How do you find the eigenvalues of a matrix and what do they represent?"
      ],
      "metadata": {
        "id": "fcZPVNO-OtWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Finding Eigenvalues in PCA:\n",
        "- Covariance Matrix\n",
        "- Eigenvalue Problem\n",
        "- Interpretation in PCA\n",
        "\n",
        "In Principal Component Analysis (PCA), the eigenvalues of a matrix play a crucial role in determining the principal components, which are the directions of maximum variance in the dataset.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "XKdudzNWOvNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q6. What are eigenvectors and how are they related to eigenvalues?"
      ],
      "metadata": {
        "id": "bAuUA5cPOvYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Eigenvectors of a matrix A are non-zero vectors v such that when A operates on v, the result is proportional to v:\n",
        "Av = λv\n",
        "Here, λ is a scalar known as the eigenvalue corresponding to the eigenvector v.\n",
        "\n",
        "\n",
        "Principal Components:\n",
        "\n",
        "- Eigenvectors in PCA are often referred to as principal components.\n",
        "- Principal components (eigenvectors) are chosen to maximize the variance explained by the data, with each principal component corresponding to an eigenvalue that represents the amount of variance explained along that component.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "K5uphdczOxW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?"
      ],
      "metadata": {
        "id": "b206JjHSOxjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Geometric Interpretation:\n",
        "\n",
        "Eigenvectors:\n",
        "- Eigenvectors in PCA represent the directions (or axes) in the original feature space along which the data has the most variance.\n",
        "- They are orthogonal (perpendicular) to each other because they correspond to different directions of maximum variance.\n",
        "- Each eigenvector vi points in a direction such that when the data points are projected onto this direction, they spread out the most.\n",
        "\n",
        "Eigenvalues:\n",
        "- Eigenvalues in PCA quantify the amount of variance in the data along the corresponding eigenvectors.\n",
        "- Larger eigenvalues correspond to eigenvectors that capture more variance in the dataset.\n",
        "- The eigenvalues are a measure of the spread or extent of the data points along the respective eigenvector directions.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_8LV90YbOzXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q8. What are some real-world applications of eigen decomposition?"
      ],
      "metadata": {
        "id": "UlMAbu91Ozmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "- Principal Component Analysis (PCA):\n",
        "Application: Dimensionality reduction in data analysis and machine learning.\n",
        "\n",
        "- Image Compression:\n",
        "Application: Image processing and compression techniques.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qr2B3_oRO1qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?"
      ],
      "metadata": {
        "id": "muFh8ogxO1y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "While a matrix can have multiple eigenvalues, each eigenvalue is associated with a set of eigenvectors. The number of linearly independent eigenvectors corresponding to each eigenvalue depends on the algebraic and geometric multiplicities of the eigenvalue.\n",
        "Eigen decomposition in PCA utilizes these eigenvectors and eigenvalues to transform and analyze high-dimensional data efficiently.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-zdv_tZwO3jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning? Discuss at least three specific applications or techniques that rely on Eigen-Decomposition."
      ],
      "metadata": {
        "id": "5DBai5r5O3qB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "- Principal Component Analysis (PCA):\n",
        "Application: Dimensionality reduction in data analysis and machine learning.\n",
        "\n",
        "- Image Compression:\n",
        "Application: Image processing and compression techniques.\n",
        "\n",
        "- Control Theory:\n",
        "Application: Control systems engineering for stability analysis.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "khjGM8lSO6yL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}